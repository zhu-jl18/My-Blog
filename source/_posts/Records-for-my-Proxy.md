---
title: Records for my Proxy and 2api Server
author:
  - mako
categories:
  - 技术记录与分享
  - AI & LLM
mathjax: false
abbrlink: 57376ce0
date: 2025-09-01T21:46:03.000Z
tags:
  - 技术分享
  - AI-LLM
  - api
  - gemini
  - ai
  - blog
  - proxy
  - workflow
updated: '2025-09-05 12:56:04'
---
>  记录大模型api反代原理，部署位置以及2api原理和部署
<!--more-->
----

## Part 1: API 反向代理

### **为什么需要反代**

  搭建API反向代理，本质上是为了解决网络问题，但其也有更深层次的价值：**安全、可控和标准化**。AI总计的核心理由如下：
   1.  **解决网络可访问性**: 这是最直接的动机，绕开location限制。
  1.  **保护API密钥**: 这是最重要的安全价值。直接调用API，API Key会暴露在前端代码中，极易被盗用。通过反代，API Key安全地存储在后端服务器，前端应用只与你的反代服务通信，由反代服务在后台添加密钥并转发请求，避免了密钥泄露的风险。详情可参考哈基米打野~
  2.  **统一接口与解决跨域 (CORS) 问题**: 进阶版的反代服务器可以作为所有API请求的统一入口，将不同服务商的API格式统一后再返回给前端。同时，它也从根本上解决了浏览器因安全策略限制网页直接请求不同域名API的跨域问题。
  3.  **成本控制与监控**: 作为一个中心化的请求枢纽，反代服务器可以轻松实现请求缓存、速率限制和日志记录等功能，帮助我们节省成本、防止恶意调用，并清晰地监控API使用情况。详情参考NEW API等。

### **反代原理**
   以 `gemini-netlify-proxy` 项目为例，其核心是一个部署在Netlify平台上的无服务器函数（Serverless Function）。它的工作流程完美诠释了反向代理的精髓，可以分解为以下四个步骤：
  
   1.  **安全准备**：函数首先从Netlify平台安全的环境变量中读取`GEMINI_API_KEY`，确保了密钥不暴露在任何公开代码中。同时，它定义了CORS（跨域资源共享）头部，允许来自任何域名的网页访问此接口。
   2.  **动态构建目标**：函数会解析访问它的URL，从中动态提取出用户希望调用的具体模型名称（如`gemini-pro-vision`）。然后，它将Google官方API的地址、模型名和后台存储的API密钥拼接成一个最终要请求的目标URL。
   3.  **请求转发**：这是反代的心脏。函数接收来自我们自己应用（如博客）的请求数据（Prompt等），然后使用服务器端的`fetch`命令，将这些数据原封不动地转发到上一步构建好的Google官方API地址。
   4.  **响应返回**：当函数收到来自Google服务器的响应后，它会立刻将这个响应的主体、状态码等关键信息，几乎原样地返回给我们自己的应用。
  
   **一言蔽之**：这个部署在云端的函数就像一个聪明的“中间人”，它接收我们的请求，安全地盖上“密钥”这个印章，然后替我们跑腿到官方API那里办事，最后再把办好的结果原封不动地带回来。

另外两个我在用的类似项目：

- **`palm-netlify-proxy`**: 这可以看作是`gemini-netlify-proxy`的“姊妹篇”。它采用了完全相同的技术原理（Netlify无服务器函数），只是将请求的目标API换成了Google的PaLM模型。这验证了该模式的可复用性。

- **`proxy-interface`**: 这是一个更强大、更通用的“反代聚合器”。它基于Node.js和Koa框架，通过路由映射，将访问不同路径（如`/openai`, `/gemini_proxy`）的请求，智能地转发到各自对应的官方API服务器。它展示了一种用单个服务统一管理所有大模型API反代的中心化思路。

### **反代平台**

要么自建**VPS (Virtual Private Server)** 租用一台云端的虚拟专用服务器来部署服务，目前我还没有涉及。要么当然是白嫖**有免费使用额度的平台** ：
- **大善人**: 
  - Deno
  - Netlify 
  - Render 
  - Cloudflare
- **其他选择**:
    - **大型云服务商**: Google Cloud (Cloud Run, Cloud Functions), AWS (Lambda)。特点是免费额度慷慨，生态强大，但配置流程相对复杂。
    - **新兴PaaS平台**: DigitalOcean App Platform。提供与Render类似的、以Git为核心的顺滑部署体验。
    - **值得关注的选项**: Heroku (PaaS模式的鼻祖，免费计划有变动), Dokku (可以在自己的VPS上搭建，实现类似Heroku的便捷部署体验)。
  
### **不同反代策略和平台的优缺点**
   不同平台的核心区别在于部署体验和抽象层次。
   - **PaaS平台 (Render, Netlify, DigitalOcean, Heroku)**: 提供“保姆式服务”。你提供代码，平台负责从构建到部署的全流程，简单快捷。这是你熟悉的“连接GitHub一键部署”模式。
   - **IaaS/FaaS平台 (AWS Lambda, Google Cloud Run)**: 提供“自助式服务”。它们运行的是标准化的“零件”（函数或容器），你需要自己定义规则（如通过CI/CD脚本）来将代码打包成这些零件。这换来了更高的灵活性和更慷慨的免费额度。
   - **自建方案 (VPS, Dokku)**: 拥有从硬件到软件的完全控制权，自由度最高，但维护成本也最高。
  
###  **目前我已经部署的反代服务**

我的反代服务主要针对 Gemini API，因为这是我主要使用的大模型服务。对于 GPT 和 Claude，我通常使用其他人提供的公益 API。

具体部署的反代服务包括：基础版反代和反截断版本


此外是针对谷歌截断问题的加强版
  
来自L站佬友们，有很多版本~~核心是两点：
- 判断响应是否被截断
- 自动重试拼接完整响应
由于 Google 已经修复了截断问题，已经处于 deprecated 状态。

  
目前我很少看到open ai和grok的反代服务。。。。


## Part 2: 2API

### 什么是 2API？

在这个语境中，"2API"特指一种通过**逆向工程 (Reverse Engineering)**，为本身没有提供公开API的Web服务（例如一个在线聊天网站），强行创造出一个可供程序化调用的API接口的技术。

**更准确的定义应该是**：2API 是一种将 Web 应用程序的前端交互逻辑转化为标准化 API 接口的技术方案。它通过分析目标网站的：

1. **网络请求流程**：抓包分析前端与后端的通信协议、请求格式和参数
2. **认证机制**：破解或复现登录状态维持、token 生成等身份验证方式
3. **数据交换格式**：理解并复现请求体的数据结构和响应解析方式
4. **业务逻辑链路**：模拟用户的完整操作流程，包括会话建立、请求发送、响应接收等

通过以上分析，2API 能够构建一个"翻译层"，将标准的 API 调用（如 OpenAI 格式）转换成目标网站能够识别的内部请求，从而实现程序化访问。

这个技术本质上是在"利用目标服务的前端作为免费的 UI，而自己掌控后端的逻辑控制"，实现了对封闭 Web 服务的 API 化改造。



这项技术的出现，其动机主要源于以下几点：
1.  **玩上最新模型**：通过逆向分析LMSYS竞技场这类平台，可以调用到那些尚未公开发布API的、最前沿的模型。
2.  **获取满血体验**：Web前端为保证用户体验，其后端接口通常功能完整，未经阉割。2API旨在调用到这个“满血版”的接口。
3.  **当然是去白嫖**：若目标Web服务本身免费，通过2API调用理论上也可以绕过官方的按量计费API。

### 2API 原理：模拟浏览器行为

https://github.com/lovingfish/OpenAI-Compatible-API-Proxy-for-Z

这个项目的核心原理是创建了一个**中间代理服务**，实现了从 OpenAI API 格式到目标 Web 服务内部格式的高效转换。

**技术架构**：
1. **标准化接口层**：代理服务提供完全兼容 OpenAI 的 `/v1/chat/completions` 等标准接口，让任何 OpenAI 客户端都能直接使用，无需修改代码。

2. **请求转换引擎**：接收到标准格式的请求后，代理会将其转换为目标网站（如 Z 平台）能够识别的内部格式。这包括：
   - 消息格式转换（OpenAI messages → Z 平台的消息结构）
   - 模型名称映射（如 gpt-4 → Z 平台内部模型 ID）
   - 参数适配（temperature、max_tokens 等参数的本地化）

3. **会话管理机制**：代理会模拟浏览器的行为，处理登录态维持、cookie 管理、token 刷新等认证相关事务，确保请求能够被目标服务正确处理。

4. **响应流式转换**：将目标服务的响应流（可能是 SSE 或其他格式）实时转换为 OpenAI 标准的流式格式，保持与官方 API 完全一致的体验。

这种设计实现了"一次接入，随处使用"的效果，用户可以像使用官方 API 一样使用这些逆向工程得到的接口。


### 当前正在使用的2API

在目前我接触和使用了两种2API：竞技场2API和智谱AI 2API。这两种都是通过逆向工程实现的非官方API接口。



竞技场2API主要针对LMSYS竞技场平台，这个平台汇集了各种前沿的大模型进行对战评估。当然拿来玩酒馆和用最新模型了。gpt-5-high非常地不错。我觉得这是一项非常伟大的发明啊~~这个佬友太厉害了。


一个小玩意，用来在cherry里和ai进行简短的涩涩...比较轻量级
- 稳定性较好，很少中断
- 破甲十分容易，很淫荡
- 输出比较快，虽然冷启动


此外，听说grok也能够通过2api进行无限制的使用，留作探索吧。




<img src = "https://media.makomako.dpdns.org/avatar/avatar.jpg" style= "width: 20% ">

